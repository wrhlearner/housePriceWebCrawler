from bs4 import BeautifulSoup
import pandas as pd
# from pyecharts.charts import Scatter
from tqdm import tqdm
import math
import requests
import lxml
import re
import time
import os
# from pyecharts.charts import Scatter
# from pyecharts import options as opts

# åŒºå
districtName = 'è”æ¹¾åŒº'
area_dic = {
            districtName :'liwan'
            # 'å¤©å®åŒº':'tianningqu',
            # 'é’Ÿæ¥¼åŒº':'zhonglouqu',
            # 'æ–°åŒ—åŒº':'xinbeiqu',
            # 'æ­¦è¿›åŒº':'wujinqu',
            # 'é‡‘å›åŒº':'jintanqu'
            }

# ä¿å­˜æ–‡ä»¶å
outputFilename = districtName + 'äºŒæ‰‹æˆ¿'

# åŠ ä¸ªheaderä»¥ç¤ºå°Šæ•¬
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Referer': 'https://gz.lianjia.com/xiaoqu/liwan/' + area_dic[districtName] + '/'}

# æ–°å»ºä¸€ä¸ªä¼šè¯
sess = requests.session()
sess.get(headers['Referer'], headers=headers)

# urlç¤ºä¾‹ï¼šhttps://sz.lianjia.com/ershoufang/luohuqu/pg2/
url = headers['Referer'] + '{}/pg{}/'

# å½“æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¤±è´¥æ—¶ï¼Œè¿”å›é»˜è®¤å€¼ï¼ˆerrifï¼‰
def re_match(re_pattern, string, errif=None):
    try:
        return re.findall(re_pattern, string)[0].strip()
    except IndexError:
        return errif

# æ–°å»ºä¸€ä¸ªDataFrameå­˜å‚¨ä¿¡æ¯
data = pd.DataFrame()

for key_, value_ in area_dic.items():
    print("start code")
    # è·å–è¯¥è¡Œæ”¿åŒºä¸‹æˆ¿æºè®°å½•æ•°
    start_url = headers['Referer'] + '{}/'.format(value_)
    html = sess.get(start_url).text
    house_num = re.findall('å…±æ‰¾åˆ°<span> (.*?) </span>ä¸ª', html)[0].strip()
    print('ğŸ’š{}: äºŒæ‰‹æˆ¿æºå…±è®¡ã€Œ{}ã€å¥—'.format(key_, house_num))
    time.sleep(1)
    # é¡µé¢é™åˆ¶ğŸš« æ¯ä¸ªè¡Œæ”¿åŒºåªèƒ½è·å–æœ€å¤š100é¡µå…±è®¡3000æ¡æˆ¿æºä¿¡æ¯
    total_page = int(math.ceil(min(3000, int(house_num)) / 30.0))
    for i in tqdm(range(total_page), desc=key_):
        html = sess.get(url.format(value_, i+1)).text
        soup = BeautifulSoup(html, 'lxml')
        info_collect = soup.find_all(class_="info clear")

        for info in info_collect:
            info_dic = {}
            # è¡Œæ”¿åŒº
            info_dic['area'] = key_
            # æˆ¿æºçš„æ ‡é¢˜
            info_dic['title'] = re_match('target="_blank">(.*?)</a><!--', str(info))
            # å°åŒºå
            info_dic['community'] = re_match('xiaoqu.*?target="_blank">(.*?)</a>', str(info))
            # ä½ç½®
            info_dic['position'] = re_match('<a href.*?target="_blank">(.*?)</a>.*?class="address">', str(info))
            # ç¨ç›¸å…³ï¼Œå¦‚æˆ¿æœ¬æ»¡5å¹´
            info_dic['tax'] = re_match('class="taxfree">(.*?)</span>', str(info))
            # æ€»ä»·
            info_dic['total_price'] = re_match('class="totalPrice"><span>(.*?)</span>ä¸‡', str(info))
            # å•ä»·
            info_dic['unit_price'] = float(re_match('data-price="(.*?)"', str(info)))

            # åŒ¹é…æˆ¿æºæ ‡ç­¾ä¿¡æ¯ï¼Œé€šè¿‡|åˆ‡å‰²
            # åŒ…æ‹¬é¢ç§¯ï¼Œæœå‘ï¼Œè£…ä¿®ç­‰ä¿¡æ¯
            icons = re.findall('class="houseIcon"></span>(.*?)</div>', str(info))[0].strip().split('|')
            info_dic['hourseType'] = icons[0].strip()
            info_dic['hourseSize'] = float(icons[1].replace('å¹³ç±³', ''))
            info_dic['direction'] = icons[2].strip()
            info_dic['fitment'] = icons[3].strip()

            # å­˜å…¥DataFrame
            if data.empty:
                data = pd.DataFrame(info_dic, index=[0])
            else:
                data = pd.concat([data, pd.DataFrame([info_dic])], ignore_index=True)

if not data.empty:
    data.to_excel(os.path.join('./data', outputFilename + '.xlsx'), index=False)

